
\section{Реализация языковых моделей}
\label{sec:arch}

Процесс создания и обучения моделей можно разделить на несколько этапов: подготовка данных, реализация модели на основе выбранного алгоритма и реализация самого цикла обучения. 

\subsection{Подготовка данных}
\label{sec:arch:data}

Для обучения моделей первоочередной задачей является сбор данных. Языковые модели в качестве данных используют текстовый корпус. Для белорусского языка нет готового открытого датасета, поэтому использовались автоматически собранные данные с белорусской Википедии и имеющиеся в открытом доступе произведения белорусской литературы. Общий объем составил 500 мб, что относительно немного. Такой объем данных не позволяет обучать большие архитектуры с нуля, однако для моделей до нескольких миллионов параметров этого вполне достаточно. Также стоит отметить, что датасет такого объема вмещается в оперативную память, что позволяет сэкономить время, которое в противном случае уходило бы на чтение с диска, а также избежать использования сложных распределенных систем обработки данных.

Далее собранные текстовые данные необходимо подготовить к использованию в моделях машинного обучения. Подготовка заключается в решении ряда задач:

\begin{enumerate}
	\item Разбиение текстового корпуса на относительно небольшие части.
	\item Удаление пунктуации и других ненужных символов, которые могут мешать при языковом моделировании.
	\item Создание словаря и обучение токенизатора.
	\item Токенизация текстов и преобразование его в вид, подходящей для использования в языковой модели.
\end{enumerate}

В теории языковые модели могут работать с текстовыми последовательностями произвольной длины. Но при обучении моделей возникает проблема в том, что для вычисления градиентов и выполнения алгоритма обратного распространения ошибок все промежуточные результаты вычислений должны сохраняться, что занимает довольно много памяти. Это накладывает достаточно серьезные ограничения на максимальную длину последовательности, особенно при разбиении обучающей выборки на батчи размера больше, чем 1. Поэтому текстовые данные приходится делить на относительно небольшие кусочки. Наиболее очевидным образом это можно сделать, разбивая их на предложения. Это наиболее интуитивный и, вероятно, наилучшия для итогового качества способ, так как в большинстве случаев связи между словами одного предложения зачастую наиболее значимые. Еще одним преимуществом данного способа является простота, так как данный алгоритм реализован в библиотеке NLTK в виде функции \text{sent\_tokenize}, принимающей на вход большой текстовый корпус и возвращяющий объект типа \text{List[str]}, содержащий полученные предложения. Общее количество предложений составило порядка 3,86 млн. Стоит также отметить, что на данном этапе выборка разбивается на тренировочную и валидационную, чтобы была возможность «честно» оценивать качество моделей. Обучающая выборка составила \text{0.85} от всего объема данных, валидационная --- \text{0.15}.

В полученных предложениях можно найти довольно много знаков препинания, которая модель может научиться предсказывать. С учетом того, что данная работа сосредоточена именно на языковом моделировании, которое не предусматривает работу с пунктуацией, то от них необходимо избавиться, так как в противном случае зависимости между словами будут более шумными и итоговое качество моделей упадет. Для избавления от пунктуации проще всего использовать решулярные выражения. В библиотеке NLTK есть спеуиальный RegexpTokenizer, который разбивает строки на части по заданному регулярному выражению. Применив его к предложениям, получаем отдельные слова, которые можно объединить через пробел для дальнейшего разбиения на токены.

Следующий шаг в обработке данных заключается в токенизации текста. Так как языковое моделирование нацелено на запоминание связей между конкретными словами или токенами, то целые предложения использовать не представляется возможным. Есть несколько вариантов разбиения текстовых последовательностей на токены: посимвольно, пословно и на subword токены, то есть токены, которые могут быть меньше целых слов, но при этом содержать более одного символа. На данный момент пословная и посимвольная токенизация используются редко. Основной недостаток посимвольного разбиения текста --- очень маленький словарь и большое количество токенов в предложениях, что делает почти невозможным работу с длинными последовательностями. У пословной токенизации проблема противоположная --- количество уникальных слов может достигать десятков и сотен тысяч, при этом значительная часть из них встречается крайне редко, и для такого большого словаря оценка вероятностей фактически невозможна. Один из способов борьбы с ланной проблемой --- наложение ограничения на длину словаря и замена неизвестных слов <UNK>. Это делает моделирование слов вне словаря невозможным. Поэтому появились способы токенизации с использованием subword алгоритмов, которые решают обе выше описанные проблемы. Наиболее популярный алгоритм subword токенизации --- Byte Pairwise Encoding (BPE) ~\cite{bpe}. В данном алгоритме словарь имеет фиксированный размер (обычно несколько тысяч) и создается с помощью оценки частоты встречаемости отдельных токенов в текстовом корпусе. Алгоритм является итеративном и постепенно оптимизирует собранный словарь, чтобы элементы в нем состояли из как можно большего количества символов.

Для создания subword токенизатора используется библиотека Sentencepiece. Она содержит реализацию алгоритма BPE и позволяет довольно легко создать токенизатор из текстового файла. Размер словаря составил 1 тысячу токенов и был выбран из имеющегося объема данных. Также в словаре есть специальные символы:

\begin{itemize}
	\item <UNK> --- для токенов, которых не поддерживаются словарем;
	\item <s> --- символ начала предложения (BOS);
	\item \text{<\textbackslash s>} --- символ конца предложения (EOS).
\end{itemize}

Полученные токены, представленные в виде индекса в словаре, могут быть использованы при обучении моделей. Для KneserNay необходимо только посчитать количество ngram, что можно сделать средствами пакета \text{nltk.lm}. Для использования в нейросетевых моделях данные оборачиваются в класс torch.Dataset. На данном этапе возникает проблема, что при обучении мы хотим собирать несколько последовательностей в один батч, и они могут быть разной длины. Тензоры же имеют фиксированные размерности. Здесь нам на помощь снова приходит класс torch.DataLoader. Он позволяет на основе объекта torch.Dataset выдавать батчи фиксированного размера. Для объединения нескольких последовательностей в батч DataLoader принимает функцию Collate. В нашем случае Collate создает тензор с размерностями, позволяющими вместить последовательность максимальной длины, а другие последовательности заполнить символами EOS. При обучении ненужные токены можно будет занулить при помощщи маски, что будет описано далее.



