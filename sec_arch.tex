
\section{Реализация языковых моделей}
\label{sec:arch}

Процесс создания и обучения моделей можно разделить на несколько этапов: подготовка данных, реализация модели на основе выбранного алгоритма и реализация самого цикла обучения. 

\subsection{Подготовка данных}
\label{sec:arch:data}

Для обучения моделей первоочередной задачей является сбор данных. Языковые модели в качестве данных используют текстовый корпус. Для белорусского языка нет готового открытого датасета, поэтому использовались автоматически собранные данные с белорусской Википедии и имеющиеся в открытом доступе произведения белорусской литературы. Общий объем составил 500 мб, что относительно немного. Такой объем данных не позволяет обучать большие архитектуры с нуля, однако для моделей до нескольких миллионов параметров этого вполне достаточно. Также стоит отметить, что датасет такого объема вмещается в оперативную память, что позволяет сэкономить время, которое в противном случае уходило бы на чтение с диска, а также избежать использования сложных распределенных систем обработки данных.

Далее собранные текстовые данные необходимо подготовить к использованию в моделях машинного обучения. Подготовка заключается в решении ряда задач:

\begin{enumerate}
	\item Разбиение текстового корпуса на относительно небольшие части.
	\item Удаление пунктуации и других ненужных символов, которые могут мешать при языковом моделировании.
	\item Создание словаря и обучение токенизатора.
	\item Токенизация текстов и преобразование их в вид, подходящий для использования в языковой модели.
\end{enumerate}

В теории языковые модели могут работать с текстовыми последовательностями произвольной длины. Но при обучении моделей возникает проблема в том, что для вычисления градиентов и выполнения алгоритма обратного распространения ошибок все промежуточные результаты вычислений должны сохраняться, что занимает довольно много памяти. Это накладывает достаточно серьезные ограничения на максимальную длину последовательности, особенно при разбиении обучающей выборки на батчи размера больше, чем 1. Поэтому текстовые данные приходится делить на относительно небольшие кусочки. Наиболее очевидным образом это можно сделать, разбивая их на предложения. Это наиболее интуитивный и, вероятно, наилучшия для итогового качества способ, так как в большинстве случаев связи между словами одного предложения зачастую наиболее значимые. Еще одним преимуществом данного способа является простота, так как данный алгоритм реализован в библиотеке NLTK в виде функции \text{sent\_tokenize}, принимающей на вход большой текстовый корпус и возвращяющий объект типа \text{List[str]}, содержащий полученные предложения. Общее количество предложений составило порядка 3,86 млн. Стоит также отметить, что на данном этапе выборка разбивается на тренировочную и валидационную, чтобы была возможность «честно» оценивать качество моделей. Обучающая выборка составила \text{0.85} от всего объема данных, валидационная --- \text{0.15}.

В полученных предложениях можно найти довольно много знаков препинания, которые модель может научиться предсказывать. С учетом того, что данные исследования сосредоточены именно на языковом моделировании, которое не предусматривает работу с пунктуацией, то от них необходимо избавиться, так как в противном случае зависимости между словами будут более шумными и итоговое качество моделей упадет. Для избавления от пунктуации проще всего использовать решулярные выражения. В библиотеке NLTK есть специальный RegexpTokenizer, который разбивает строки на части по заданному регулярному выражению. Применив его к предложениям, получаем отдельные слова, которые можно объединить через пробел для дальнейшего разбиения на токены.

Следующий шаг в обработке данных заключается в токенизации текста. Так как языковое моделирование нацелено на запоминание связей между конкретными словами или токенами, то целые предложения использовать не представляется возможным. Есть несколько вариантов разбиения текстовых последовательностей на токены: посимвольно, пословно и на subword токены, то есть токены, которые могут быть меньше целых слов, но при этом содержать более одного символа. На данный момент пословная и посимвольная токенизация используются редко. Основной недостаток посимвольного разбиения текста --- очень маленький словарь и большое количество токенов в предложениях, что делает почти невозможным работу с длинными последовательностями. У пословной токенизации проблема противоположная --- количество уникальных слов может достигать десятков и сотен тысяч, при этом значительная часть из них встречается крайне редко, и для такого большого словаря оценка вероятностей фактически невозможна. Один из способов борьбы с данной проблемой --- наложение ограничения на длину словаря и замена неизвестных слов <UNK>. Это делает моделирование слов вне словаря невозможным. Поэтому появились способы токенизации с использованием subword алгоритмов, которые решают обе выше описанные проблемы. Наиболее популярный алгоритм subword токенизации --- Byte Pairwise Encoding (BPE) ~\cite{bpe}. В данном алгоритме словарь имеет фиксированный размер (обычно несколько тысяч) и создается с помощью оценки частоты встречаемости отдельных токенов в текстовом корпусе. Алгоритм является итеративном и постепенно оптимизирует собранный словарь, чтобы элементы в нем состояли из как можно большего количества символов.

Для создания subword токенизатора используется библиотека Sentencepiece. Она содержит реализацию алгоритма BPE и позволяет довольно легко создать токенизатор из текстового файла. Размер словаря составил 1 тысячу токенов и был выбран из имеющегося объема данных. Также в словаре есть специальные символы:

\begin{itemize}
	\item <UNK> --- для токенов, которых не поддерживаются словарем;
	\item <s> --- символ начала предложения (BOS);
	\item \text{<\textbackslash s>} --- символ конца предложения (EOS).
\end{itemize}

Полученные токены, представленные в виде индекса в словаре, могут быть использованы при обучении моделей. Для KneserNay необходимо только посчитать количество ngram, что можно сделать средствами пакета \text{nltk.lm}. Для использования в нейросетевых моделях данные оборачиваются в класс torch.Dataset. На данном этапе возникает проблема, что при обучении мы хотим собирать несколько последовательностей в один батч, и они могут быть разной длины. Тензоры же имеют фиксированные размерности. Здесь нам на помощь снова приходит класс torch.DataLoader. Он позволяет на основе объекта torch.Dataset выдавать батчи фиксированного размера. Для объединения нескольких последовательностей в батч DataLoader принимает функцию Collate (Листинг \ref{lst:arch:collate}). В нашем случае Collate создает тензор с размерностями, позволяющими вместить последовательность максимальной длины, а другие последовательности заполнить символами EOS. При обучении ненужные токены можно будет занулить при помощи маски, что будет описано далее.

\begin{lstlisting}[caption={Функция collate для сбора последовательностей различной длины в батч}, label=lst:arch:collate]
def collate(sequences: tp.List[int]) -> torch.LongTensor:
	sequence_lengths = [len(seq) for seq in sequences]
	max_len = max(sequence_lengths)
	for seq in sequences:
		seq.extend([EOS_ID] * (max_len - len(seq)))
		assert len(seq) == max_len
	return torch.LongTensor(sequences)    
\end{lstlisting}

\subsection{Реализация и обучение моделей}

\subsubsection{Kneser-Nay} Модель Kneser-Nay по своей сущности представляет собой набор статистической информации о тренировочном текстовом корпусе. Для создания модели необходимо разбить текст на n-граммы, что можно сделать с помощью функции nltk.ngrams. Далее полученные n-граммы подаются в класс KneserNayInterpolated для получения итоговой модели.

\subsubsection{LSTM}. Реализация включает в себя:

\begin{enumerate}
	\item код реализации самой модели;
	\item вычисление функции потерь;
	\item реализация цикла обучения;
	\item оценка и логирование результатов.
\end{enumerate}

Как уже было сказано ранее, основная причина использования библиотеки Pytorch при построении языковых моделей --- это возможность автоматического выполнения обратного распространения ошибки. Для того, чтобы реализованную архитектуру можно было интегрировать со всеми средствами Pytorch, включая встроенные инструменты расчета функции потерь и оптимизации градиентного спуска, лежащего в основе обучения нейросетевых моделей, необходимо ее описывать в виде класса, наследующего класс torch.nn.Module. Базовая архитектура языковой модели на основе LSTM представлена в листинге \ref{lst:arch:lstm}.

\begin{lstlisting}[caption={Реализация простой языковой модели на основе LSTM}, label=lst:arch:lstm]
class RNNLanguageModel(nn.Module):
	def __init__(self, vocab_size, num_layers, emb_size=384, hidden_size=384, dropout=0.1):
		super().__init__()
		sub-layers, trainable variables, etc.
		self.emb = nn.Embedding(vocab_size, emb_size)
		self.lstm = nn.LSTM(emb_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
		self.dense = nn.Linear(hidden_size, vocab_size)
	
	def forward(self, input_ix):
		sequence = self.emb(input_ix)
		output, _ = self.lstm(sequence)
		return self.dense(output)  
\end{lstlisting}

Данная модель состоит из трех основных слоев: nn.Embedding, nn.LSTM и nn.Linear.

nn.Embedding --- слой эмбеддингов. Цель данного слоя заключается в получении для каждого токена его векторного представления. В нем содержится одна матрица с количеством параметров, равным произведению объема словаря и размера векторного представления. Данные параметры являются обучаемыми и изменяются во время тренировки. Для многих задач применяются представления, полученные на огромных массивах текстовых данных, так как это значительно улучшает качество и ускоряет обучение. Однако для белорусского языка нет качественных эмбеддингов, поэтому необходимо их учить с нуля. На вход данный слой получает целочисленный тензор размера batch\_size $\times$ sequence\_length, а выдает тензор с вещественными числами размера batch\_size $\times$ sequence\_length $\times$ embedding\_size, который подается на вход рекуррентному модулю.
	
nn.LSTM --- рекуррентный модуль, реализующий алгоритм LSTM ~\cite{lstm}. Стоит уточнить, что один слой nn.LSTM может содержать более одного компонента nn.LSTMСell, который и содержит в себе весь механизм LSTM. Данный слой является основным для рекуррентной языковой модели, и от его параметров зависят ее основные свойства. К параметрам относятся:
\begin{itemize}
	\item количество рекуррентных ячеек LSTMCell (num\_layers),
	\item размер вектора состояния (hidden\_size), 
	\item размер векторного представления токенов (emb\_size), 
	\item вероятность dropout, 
	\item флаг bilinear, который определяет, является ли сеть двунаправленной.
\end{itemize}

Для генерации текстовых последовательностей двунаправленные сети не используются, так как необходимо учитывать исключительно предыдущие токены, чтобы определить следующий. Двунаправленные сети обычно применяются для получения более качественного представления текста в виде вектора. 

Параметр dropout используется в случае, если сеть содержит более одной ячейки LSTMCell. В данном случае можно улучшить процесс обучения с помощью слоя nn.Dropout. Данный слой во время тренировки с некоторой вероятностью зануляет выходы нейронов, что имеет регуляризующий эффект. Это снижает переобучаемость модели и позволяет улучшить итоговое качество. Однако слишком большая вероятность приводит к тому, что модель не модет запомнить никаких зависимостей, поэтому обычно используется значение в пределах 0.3, а в данной работе --- 0.1.

Наибольшее влияние на архитектуру модели оказывает параметр hidden\_size. Он определяет, какой объем информации о предыдущих токенах модель способна хранить. Слишком маленький размер скрытого состояния приводит к тому, что модель быстро забывает предыдущие токены, а чрезмерно большой не позволяет достаточно хорошо подобрать его параметры. Поиск оптимального размера скрытого слоя, наравне с количеством элементов LSTMCell, будут основным направлением экспериментов с моделями LSTM.

На вход слой LSTM принимает выход nn.Embedding --- трехмерный тензор batch\_size $\times$ sequence\_length $\times$ embedding\_size. На выходе имеем два вектора: первый описывает выходное состояние, второй является вектором скрытого состояния. В нашем случае будет использоваться только вектор выходного состояния, так как именно в нем содержится информация о следующем токене. В векторе скрытого состояния сохранена вся предыдущая последовательность, что не поможет при предсказании следующего элемента последовательности. Размер выходного состояния batch\_size $\times$ sequence\_length $\times$ hidden\_size. 

Слой nn.Linear в языковой модели необходим для получения логитов из выходного состояния LSTM. Логиты представляют собой вещественные числа, принимающие значения от $-\infty$ до $+\infty$, и являются биективным отображением вероятностного пространства. Логиты можно получить из вероятностей по формуле \ref{eq:logit}. Использование такого представления вероятностей позволяет сводить задачу классификации к регрессии, которую способны решать нейросети.

\begin{equation}
	logit(p) = \sigma^{-1}(p) = \ln{\left(\frac{p}{1 - p}\right)}
	\label{eq:logit}
\end{equation}

В нашем случае линейный слой преобразует вход с размером вектора hid\_size к размеру словаря, чтобы была возможность выбирать наиболее вероятный следующий токен. Далее для получения конкретных вероятностей к логитам можно применить функцию Softmax (\ref{eq:softmax}), которая переводит значения из области всех вещественных чисел в отрезок [0, 1]. Также Softmax используется для кросс-энтропии, которая применяется в качестве функции потерь при языковом моделировании.

\begin{equation}
	Softmax(x_k) = \frac{e^{x_k}}{\sum_{i=1}^{n}{e^{x_i}}}
	\label{eq:softmax}
\end{equation}

Наиболее широко используемая функция потерь для языкового моделирования --- это кросс-энтропия. В Pytorch это представлено в виде класса nn.CrossEntropyLoss. Данный класс при вызове принимает тензор логитов и тензор с метками, далее вычисляет от логитов Softmax и рассчитывает loss по формуле (\ref{eq:cross_entropy}). Что происходит далее, зависит от режима reduction. В зависимости от задачи, можно выбрать один из трех вариантов:

\begin{enumerate}
	\item mean --- на выходе результаты по всем элементам усредняются;
	\item sum --- результаты по всем элементам складываются,
	\item none --- возвращает значения в том же виде, что и входной тензор.
\end{enumerate}

Для большинства задач, особенно для классификации, используется режим mean. В нашем случае это невозможно из-за того, что последовательности имеют различную длину и они дополнены искусственными символами, которые на самом деле не должны влиять на градиенты.

Чтобы можно было обрабатывать большие батчи, необходимо учитывать переменную длину последовательностей. Это выполняется при помощи масок. Логиты из модели получаются все еще для всех токенов, включая ненастоящие, но при подаче в nn.CrossEntropyLoss они поэлементно умножаются на маску, что нивелирует вклад искусственно добавленых токенов. Полученные значения суммируются и делятся на общее количество токенов в последовательностях, или количество единиц в матрице маски. Код функции потерь приведен в листинге ~\ref{lst:arch:loss}. Стоит также отметить, что на вход модели поступает матрица с последовательностями без последнего столбца, а в функцию потерь в качестве целевых значений --- без первого. Это значит, что таргеты являются смещением на один токен входных данных, что соответствует решаемой языковыми моделями задаче.

\begin{lstlisting}[caption={Расчет функции потерь}, label=lst:arch:loss]
def compute_loss(self, batch):
	inp = batch[:, :-1]
	out = batch[:, 1:]
	mask = inp != EOS_ID
	logits = self(inp).transpose(1, 2)
	loss = torch.sum(self.criterion(logits, out) * mask) / torch.sum(mask)
	return loss
\end{lstlisting}

Само по себе значение функции потерь полезно только для оценки качества модели. Чтобы оптимизировать ее веса, необходимо получить ее градиенты и выполнить шаг оптимизатора. Градиенты в PyTorch получаются автоматически, при вызове метода backward() у значения loss все обучаемые параметры автоматически получают соответствующие значения. Однако чтобы их использовать для оптимизации весов модели, необходим оптимизатор. Основная идея заключается в том, что при использовании полных значений градиентов при градиентном спуске, шаги в пространстве весов получаются слишком большие, что делает невозможным сходимость к минимуму оптимизируемой функции. Оптимизатор совершает шаги с определенным «темпом», который называется learning rate (скорость обучения). Самый простой процесс оптимизации описывается формулой \ref{eq:gradient_descent}. Есть более сложные оптимизаторы, например, Adam и AdamW, которые учитывают предыдущий шаг градиента, но способ применения один и тот же --- вызов метода step после loss.backward(). Далее выполняется optimizer.zero\_grad(), чтобы занулить все градиенты, так как при повторном вызова backward() они суммируются.

\begin{equation}
	x = x - \alpha \frac{\partial y}{\partial x}
	\label{eq:gradient_descent}
\end{equation}
\begin{explanation}
	где x --- оптимизируемый вес модели,
	y --- значение функции потерь (либо \\ градиент на следующем слое),
	$\alpha$ --- learning rate.
\end{explanation}

Цикл обучения представляет собой процесс, при котором происходит оптимизация весов модели для решения конкретной задачи. Основными частями цикла обучения являются шаг тренировки и шаг валидации. На шаге тренировке выполняются следующие действия:

\begin{itemize}
	\item получение батча из train\_dataloader;
	\item получение логитов из модели и расчет функции потерь;
	\item расчет градиентов посредством вызова loss.backward();
	\item выполнение шага оптимизатора;
	\item логирование значения функции потерь.
\end{itemize}

Шаг валидации аналогичен шагу тренировки, однако выполняется без расчета градиентов и вызова оптимизатора. Полученные метрики в дальнейшем используются для оценки качества модели и осуществления контроля над процессом обучении. В случае, если значение лосса на валидации не улучшается, то либо было достигнуто наилучшее качество с имеющимся набором данных и выбранной архитектурой, либо learning rate слишком большой для дальнейшей оптимизации. С данной проблемой также помогают бороться шедулеры --- объекты, изменяющие значения learning rate при определенных условиях. Примеры шедулеров:

\begin{itemize}
	\item ReduceLROnPlateu --- снижает learning rate, если за определенное количество итераций не улучшилась определенная метрика.
	\item CosineAnnealingLR --- learning rate изменяется по косинусоиде.
	\item ExponentialLR --- learning rate уменьшается экспоненциально через заданное количество итераций.
	\item OneCycleLR --- learning rate изменяется циклически.
\end{itemize}

Цикл обучения выполняется при помощь класса Trainer из библиотеки Pytorch Lightning. В нем содержится множество функционала для тренировки, валидации и тестирования моделей, встроенная возможность запуска на нескольких видеокартах, встроенное логирование и возможность для максимально простого хранения чекпоинтов модели. Также в Pytorch Lightning есть поддержка различных способов логирования, один из которых --- WandbLogger, который в совокупности с библиотекой wandb предоставляет веб-интерфейс с тензорбордами, в который можно наблюдать за ходом тренировок. В данной работе логируются значения функции потерь на этапе тренировки и во время валидации.

При выполнении предыдущих этапов будет получена модель, которая способна принимать текстовую последовательность и выдавать для нее наиболее вероятный следующий токен. Также такая модель может оценить вероятность заданной последовательности. Однако чтобы генерировать более длинные предложения, этого может быть недостаточно.

При генерации последовательности длины $n$, существует $|V|^{n}$ возможных вариантов, где $|V|$ --- размер словаря. Очевидно, что такое количество последовательностей оценить невозможно. В связи с этим для генерации текста используются неоптимальные алгоритмы, дающие достаточно неплохие результаты. Наиболее часто используемыми приближенными алгоритмами являются жадный алгоритм и beam search ~\cite{beam_search}.

Наиболее простым способом генерации текста является жадный алгоритм. В данном алгоритме на каждом шаге выбирается токен с наибольшей вероятностью и добавляется в выходную последовательность. Далее он подается на вход модели, и действия повторяются. Однако данный способ не дает превосходного качества, так как оптимальный на определенном шаге токен совсем необязательно будет в наилучшей последовательности. Достаточно часто складывается ситуация, при которой если на определенном шаге взять менее вероятный токен, то следующий шаг это скомпенсирует, и итоговый результат будет лучше. Чтобы это учитывать, создан алгоритм beam search.

В beam search на каждом шаге хранится не одна последовательность, а несколько. Далее для каждого луча рассчитывается распределение вероятностей для следующего токена, затем рассчитываются суммарные вероятности для топ-k последовательностей в каждом луче. Все последовательности совместно вортируются, и снова выбирают топ-k обновленных последовательностей, которые становятся новыми лучами. Таким образом, сложность такого алгоритма зависит от размера словаря линейно, а не экспоненциально, и качество генерируемых последовательностей лучше, чем для жадного алгоритма.

Для оценки результатов используются различные метрики. Основная метрика для генерации текстов --- это perplexity. Данная метрика описывает, насколько модель «уверена» в выборе следующего токена, и вытекает напрямую из кросс-энтропии. Perplexity рассчитывается по формуле (\ref{eq:perplexity}). 

\begin{equation}
	PP(W) = 2^{H(W)} = 2^{-\frac{1}{N}log_2P(w_1, w_2,\dots,w_n)}
	\label{eq:perplexity}
\end{equation}

Сравнив значения perplexity для различных моделей, можно будет сделать вывод, какая архитектура лучше всего подходит для поставленной задачи, и какие параметры моделей являются оптимальными.

\subsection{Трансформеры}

Модели трансформеров содержатся в библиотеке transformers от HuggingFace. В ней предоставлено довольно большое количество различных моделей, включая языковые. Архитектура, используемая в данной работе --- GPT2 ~\cite{gpt2}. Стоит отметить, что у gpt2 есть несколько вариаций, содержащих от десятков миллионов параметров, до более миллиарда. Так как объем имеющегося текста меньше 1 Гб, то наиболее логично использовать наименьшую модель --- distill gpt2, содержащую порядка 30 миллионов параметров.

Модель GPT2 идет совместно с токенизатором на более чем 30 тысяч токенов, которые необходимо воссоздать из имеющихся данных. Также весь алгоритм тренировки, включая оптимизацию весов, логирование результатов и распараллеливание вычислений, реализован в классе Trainer от huggingface, что значительно облегчает проведение экспериментов.

Архитектура GPT-2 состоит слоя эмбеддингов, большого количества блоков трансформеров и линейного слоя для получения логитов. Это во многом схоже с LSTM по структуре, но значительно отличается по принципу работы и свойствам.

Сам процесс обучения ничем не отличается от LSTM. Основная разница заключается во времени и ресурсах, затрачиваемых на весь процесс. Архитектура тренсформеров ~\cite{attention-is-all-you-need} работает за квадратичное от длины последовательностей время и требует больше памяти на обучение, что сильно сказывается на производительности.

В течение достаточно продолжительного времени GPT-2 была state-of-the-art моделью языкового моделирования. Ожидается, что результаты для нее будут лучше, чем для других архитектур, однако этого может не произойти из-за ограниченного объема тренировочных данных.

