\section{Обзор предметной области}
\label{sec:domain}

В данном разделе будет произведён обзор предметной области задачи, решаемой в рамках дипломного проекта; рассмотрены алгоритмы для работы с текстовыми данными и развитие технологмй в данном направлении. Также будут рассмотрены подходы к решению задачи языкового моделирования.

\subsection{Natural Language Processing}
\label{sub:domain:nlp}

Natural Language Processing (NLP) --- область машинного обучения, связанная с предоставлением компьютерам возможности понимать текст и произносимые слова почти так же, как люди.

NLP сочетает в себе вычислительную лингвистику — моделирование человеческого языка на основе правил — со статистическими моделями, машинным обучением и моделями глубокого обучения. Вместе эти технологии позволяют компьютерам обрабатывать человеческий язык в виде текстовых или голосовых данных и «понимать» его полное значение, включая намерения и чувства говорящего или пишущего.

NLP управляет компьютерными программами, которые переводят текст с одного языка на другой, реагируют на голосовые команды и быстро резюмируют большие объемы текста — даже в режиме реального времени. В повседневной жизни многие взаимодействовали с NLP в виде голосовых систем GPS, цифровых помощников, программного обеспечения для преобразования речи в текст, чат-ботов для обслуживания клиентов и других сервисов. Но NLP также играет все более важную роль в корпоративных решениях, помогающих упростить бизнес-операции, повысить производительность сотрудников и упростить критически важные бизнес-процессы.

Человеческий язык полон двусмысленностей, которые невероятно затрудняют написание программного обеспечения, точно определяющего предполагаемое значение текстовых или голосовых данных. Омонимы, омофоны, сарказм, идиомы, метафоры, исключения из грамматики и использования, вариации в структуре предложений — это лишь некоторые из особенностей человеческого языка, на изучение которых у людей уходят годы, однако приложения должны изначально уметь это распознавать, чтобы приносить какую-либо пользу.

Некоторые задачи NLP требует обработки одновременно и текстовых, и гоосовых данных. К ним относятся: 

\begin{itemize}
	\item Распознавание речи, что представляет собой задачу надежного преобразования голосовых данных в текстовые. Распознавание речи требуется для любого приложения, которое следует голосовым командам или отвечает на голосовые запросы. Что делает распознавание речи особенно сложным, так это то, как люди говорят: быстро, невнятно, с разным ударением и интонацией, с разным акцентом и часто с использованием неправильной грамматики.
	
	\item Тегирование частей речи, также называемое грамматическим тегированием, представляет собой процесс распознавания части речи определенного слова или фрагмента текста на основе его использования и контекста.
	
	\item Устранение неоднозначности смысла слова --- это выбор значения слова с несколькими значениями посредством процесса семантического анализа, который определяет слово, которое имеет наибольший смысл в данном контексте.
	
	\item Распознавание именованных сущностей, или Named Entity Recognition (NER), идентифицирует слова или фразы как полезные сущности. NER позволяет по слову и его контексту понять, это имя человека, город или какой-либо иной объект.
	
	\item Анализ эмоциональной окраски пытается извлечь из текста субъективные качества — отношение, эмоции, сарказм, замешательство, подозрительность.
	
	\item Генерация естественного языка ---  это задача перевода структурированной информации на человеческий язык.~\cite{ibm_nlp}
\end{itemize}

\subsection{Задача языкового моделирования}
\label{sub:domain:language_modeling}

Языковое моделирование (Language Modeling или LM) — это использование различных методов для определения вероятности появления данной последовательности слов в предложении. Языковые модели анализируют массивы текстовых данных, чтобы обеспечить основу для своих предсказаний. Они используются для решения задач, связанных с обработкой естественного языка и при генерации текста. К таким задачам среди прочих относятся машинный перевод и ответы на вопросы.

Основная цель в языковом моделировании --- оценить вероятности текстовых фрагментов. Мы хотим, чтобы эти вероятности отражали знание языка. В частности, мы хотим, чтобы предложения, которые «с большей вероятностью» появятся в языке, имели большую вероятность в соответствии с нашей языковой моделью. Вероятности последовательностей оцениваются по формуле условной вероятности (\ref{eq:conditional_proba}):
\begin{equation}
	P(y_1, y_2,\dots,y_n) = P(y_1) \cdot P(y_2|y_1) \cdot P(y_3|y_1, y_2) \cdot \dots \cdot P(y_n|y_1,\dots,y_{n-1}) =
	\prod_{t=1}^n{P(y_t|y_{<t})}
	\label{eq:conditional_proba}
\end{equation}
Благодаря данной формуле мы можем преобразовать вероятность последовательности в произведение условных вероятностей отдельных слов или токенов.~\cite{lena-voita-lm}

Языковые модели определяют вероятность слова, анализируя текстовые данные. Они интерпретируют эти данные, пропуская их через алгоритм для получения контекстной информации. Затем модель применяет эту информацию в языковых задачах, чтобы точно прогнозировать или создавать новые предложения. Модель по существу изучает особенности и характеристики языка и использует эти знания для понимания новых фраз.

Методы языкового моделирования:
\begin{itemize}
	\item N-грамм. N-граммы — это относительно простой подход к языковым моделям. Они создают распределение вероятностей для последовательност длины $n$. $n$ может быть любым числом и определяет размер последовательности слов, которым присваивается вероятность. По сути, $n$ можно рассматривать как объем контекста, который модель должна учитывать. Некоторые типы n-грамм — это униграммы, биграммы, триграммы и так далее.
	
	\item Униграмма. Униграмма — простейший тип языковой модели. В вычислениях не рассматривается какой-либо контекст. Модель оценивает каждое слово или термин независимо. Модели Unigram обычно решают задачи обработки языка, такие как поиск информации. Униграмма является основой более конкретного варианта модели, называемой моделью вероятности запроса, которая использует поиск информации для изучения пула документов и поиска наиболее релевантного для конкретного запроса.
	
	\item Двунаправленный. В отличие от моделей n-грамм, которые анализируют текст в одном направлении (назад), двунаправленные модели анализируют текст в обоих направлениях, назад и вперед. Эти модели могут предсказать любое слово в предложении или теле текста. Двунаправленное изучение текста повышает точность результатов. Этот тип часто используется в приложениях для машинного обучения и генерации речи. Например, Google использует двунаправленную модель для обработки поисковых запросов. Однако такие модели не подходят для генерации текстовых последовательностей.
	
	\item Экспоненциальный. Этот тип, также известный как модели максимальной энтропии, сложнее, чем n-граммы. Проще говоря, модель оценивает текст с помощью уравнения, которое объединяет функции признаков и n-граммы. По сути, этот тип определяет характеристики и параметры желаемых результатов и, в отличие от n-грамм, оставляет параметры анализа более неоднозначными — например, он не указывает размеры отдельных граммов. Модель основана на принципе энтропии, согласно которому распределение вероятностей с наибольшей энтропией является наилучшим выбором. Другими словами, модель с наибольшим хаосом и наименьшим количеством предположений является наиболее точной. Экспоненциальные модели основаны на максимизации кросс-энтропии, которая сводит к минимуму количество статистических предположений.
	
	\item Нейросетевой. Этот тип модели представляет слова как нелинейную комбинацию весов в нейронной сети. Веса слов называются эмбеддингами (embedding). Этот тип становится особенно полезным по мере того, как наборы данных становятся все больше, потому что большие наборы данных часто включают больше уникальных слов. Наличие большого количества уникальных или редко используемых слов может вызвать проблемы для линейной модели, такой как n-грамма. Это связано с тем, что количество возможных последовательностей слов увеличивается, а шаблоны, определяющие результаты, становятся слабее. Взвешивая слова нелинейным, распределенным способом, эта модель может «научиться» аппроксимировать слова и, следовательно, не будет введена в заблуждение какими-либо неизвестными значениями. Его «понимание» данного слова не так сильно привязано к непосредственно окружающим словам, как в моделях n-грамм.
\end{itemize}

Языковое моделирование имеет решающее значение при решении современных задач анализа текстовых данных. Именно благодаря им машины могут понимать качественную информацию. Каждый тип языковой модели тем или иным образом превращает качественную информацию в количественную. Это позволяет людям в ограниченной степени общаться с машинами так же, как они общаются друг с другом. 

Языковые модели применяются в различных отраслях, включая технологии, финансы, здравоохранение, транспорт, юриспруденцию и другие. Кроме того большинство людей, каким-то образом взаимодействуют с языковой моделью в на ежедневной основе, будь то поиск Google, функция автозаполнения текста или взаимодействие с голосовым помощником.~\cite{language_modeling_abstract}

\subsection{История языкового моделирования}

Языковое моделирование имеет довольно насыщенную историю. Впервые n-грамные модели упоминались в статье~\cite{shannon-math-theory-of-communication} еще в 1948 году, где описывалось применение марковских цепей для создания статистической модели для последовательностей символов в сообщениях на английском языке. В 1970 году финским студентом Seppo Linnainmaa был предложен в современном виде алгоритм backpropagation~\cite{backprop_first_mention}, который в дальнейшем ляжет в основу 99.9\% всех нейросетей.

В 1980 годах основные исследования все еще были сосредоточены на статистических методах, однако вместе с этим появилось несколько методов нейросетевого моделирования, которые найдут применение в дальнейшем. В 1982 появилась идея рекуррентных нейросетей, описанных в статье J. Hopfield \cite{hopfield_rnn}. Данные сети не имеют ничего общего с современными рекуррентными нейросетями, так как не способны обрабатывать последовательности данных, однако заложили основу для дальнейших исследований. В 1986 году более глубоко рассматривалось применение backpropagation (или метод обратного распространения ошибки) в рекуррентных сетях для генерации последовательностей~\cite{rumelhart_backprop}. Также возникла идея представления различных символом или слов в виде векторов. Однако полноценной нейросетевой моделью для языкового моделирования это считать нельзя, так как эксперименты проводились на искусственных данных и с малым количеством различных символов.

В 1990 годах количество исследований в сфере NLP значительно выросло. N-грамные модели во всю использовались для решения задач на текстовых данных. Были придуманы различные подходы для улучшения или «сглаживания» вероятностей на выходе моделей, что значительно улучшило их качество. Появились такие подходы, как Kneser-Ney Smoothing~\cite{kneser_ney}, который и сейчас находит себе применение в случаях, когда нет необходимости учитывать длинные контексты или нет возможности эффективно применять нейросетевые LM из-за ограничений по памяти или вычислительным ресурсам. В 1990 году языковые модели впервые применены к задаче машинного перевода~\cite{mt_first}. В ней описано применение n-грамных моделей для перевода с французского языка на английский. В 1997 была придумана архитектура LSTM ~\cite{lstm}, которая с развитием нейронных сетей на какое-то время станет лучшей моделью языкового моделирования. Благодаря данной архитектуре улучшается способность нейронных сетей запоминать информацию о более длинных последовательностей.

В начале 2000-ых появились первые нейросети для языкового моделирования. В 2003 году вышла статья A Neural Probabilistic Language Model~\cite{first_neural_lm}, в которой описана полносвязная нейросеть, которая по контексту фиксированной длины позволяет предсказывать следующее слово. Каждое слово представляет собой вектор фиксированной длины, который обучается вместе с основной моделью. В дальнейшем такие векторные представления слов приведут к появлению Word Embeddings. Схема нейронной сети представлена на рисунке \ref{fig:first_neural_lm}.

\begin{figure}[ht]
\centering
	\includegraphics[width=\linewidth]{first_neural_lm}
	\captionof{figure}{Схема первой нейросетевой языковой модели}
	\label{fig:first_neural_lm}
\end{figure}

Следующий прорыв в языковом моделировании случился в 2013 году. Это связано с появлением Word2Vec --- универсальными векторными представлениями слов, разработанными Google. Векторные представления обучены на большом объеме данных, что позволяет хорошо отражать смысл слов. Это значит, что схожим по смыслу и значения словам будут соответствовать близкие по косиноснуму расстоянию вектора. Также исследования показали, что такие представления имеют некоторую линейную зависимость, которая выражается в подобных примерах: $king - man + woman \approx queen$.
Представления Word2Vec могут быть получены на больших объемах неразмеченных текстовых данных с помощью двух методов: Skip-gram и Continous Bag of Words (CBoW). Skip-gram основан на предсказании по данному слову его контекста, а CBoW наоборот --- пытается угадать слово по заданному контексту (Рис. \ref{fig:skipgram_cbow}). Также есть способы улучшить качество векторов и скорость сходимости, например, negative sampling, при котором кроме приближения схожих слов семплируются несколько случайных, и алгоритм пытается «отдалить» друг от друга слова, отличающиеся по смыслу.

\begin{figure}[ht]
	\includegraphics[width=\linewidth]{skip_gram_cbow}
	\captionof{figure}{Skip-gram и CBoW}
	\label{fig:skipgram_cbow}
\end{figure}

Через год после успеха Word2Vec появился GloVe --- аналогичные векторные представления слов, которые показали себя лучше для ряда задач: поиск схожих слов и NER (Рис. ~\ref{fig:glove_vs_word2vec}). GloVe отличается исходными данными для обучения, а также основан на более сложном алгоритме.~\cite{twds-lm-history}

\begin{figure}[ht]
	\includegraphics[width=\linewidth]{glove_vs_word2vec}
	\captionof{figure}{GloVe vs Word2Vec}
	\label{fig:glove_vs_word2vec}
\end{figure}

Одновременно с GloVe в 2014 году были созданы сверточные нейронные сети ~\cite{kalchbrenner-etal-2014-convolutional}. Основной сферой применения сверточных сетей является компьютерное зрение, так как по своей природе они наиболее удачно подходят для обработки изображений, однако их можно применять и к текстовым данным. Наряду с рекуррентными сетями, сверточные сети были одним из основных направлений исследований в NLP.

Несмотря на то, что рекуррентные сети являются наиболее очевидным выбором для работы с последовательностями переменной длины, их обучение затруднялось особенностями архитектуры, а также такими феноменами, как затухающие и взрывающиеся градиенты, которые влиялии на вычислительную стабильность обратного распространения градиентов. Довольно большой вклад в решение данных проблем внес phd тезис~\cite{sutskever2013training}, в котором приводились результаты успешных экспериментов для различных задач, не только языкового моделирования, которые доказали возможность качественного обучения рекуррентных сетей.

Следующим шагом в развитии моделей для обработки последовательностей были Sequence to Sequence модели (seq2seq). В этой структуре нейронная сеть кодировщика обрабатывает предложение по одному токену и сжимает его в векторное представление; затем нейронная сеть декодера предсказывает выходные символы на основе состояния кодировщика, принимая в качестве входных данных на каждом шаге ранее предсказанный символ (Рис. \ref{fig:seq2seq})

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{seq2seq2014}  
	\caption{ Схема seq2seq модели }
	\label{fig:seq2seq}
\end{figure}

Seq2seq благодаря своей гибкости в настоящее время является основным фреймворком для задач генерации естественного языка, при этом различные модели берут на себя роль кодировщика и декодера. Важно отметить, что модель декодера может быть обусловлена не только последовательностью, но и произвольными представлениями. Это позволяет, например, генерировать заголовок на основе изображения (Image Captioning)~\cite{image-captioning}. Здесь в качестве кодировщика применяется сверточная сеть, которая выдает сжатые представления изображений. Далее этот вектор идет на вход декодера, который выдает одно за другим слова заголовка (Рис \ref{fig:image_captioning}).
\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{image_captioning}  
	\caption{ Архитектура seq2seq модели для генерации заголовков к изображениям }
	\label{fig:image_captioning}
\end{figure}

У модели seq2seq есть один значитеьный недостаток, который ограничивает качество моделей, --- вся входная информация сжимается в один вектор, который в дальнейшем используется в рекуррентной сети. Это приводит к тому, что после нескольких применений декодера информация о входном объекте теряется, что ограничивает качество модели. С этим борется механизм внимания, или Attention.

Механизм внимания является одним из основных нововведений в нейронном машинном переводе (NMT) и ключевой идеей, которая позволила моделям NMT превзойти классические системы машинного перевода на основе фраз. Attention позволил решить основную проблему seq2seq моделей, предоставляя декодеру возможность оглянуться назад на скрытые состояния исходной последовательности, которые затем предоставляются в качестве дополнительных входных данных для декодера (Рис. \ref{fig:attention}).

\begin{figure}[ht]
	\centering
	\includegraphics[width=200pt]{attention}  
	\caption{ Механизм attention }
	\label{fig:attention}
\end{figure}

Формула вероятности $P(y_i|y_{<i}, x)$, где $x$ --- это выход кодировщика, для модели attention имеет вид (\ref{eq:attention_1}):
\begin{equation}
	P(y_i|y_1,\dots,y_{i-1}, x) = g(y_{i-1}, s_i, c_i),
	\label{eq:attention_1}
\end{equation} где $s_i$ --- это последнее скрытое состояние рекуррентной сети, а вектор $c_i$ равен взвешенной сумме скрытых состояний рекуррентной сети (\ref{eq:attention_2} -- \ref{eq:attention_4}).

\begin{equation}
	c_i = \sum_{j=1}^{T_x}{\alpha_{ij}h_j}
	\label{eq:attention_2}
\end{equation}

\begin{equation}
	a_{i,j} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_x}{e_{ik}}}
	\label{eq:attention_3}
\end{equation}

\begin{equation}
	e_{ij} = a(s_{i-1}h_j)
	\label{eq:attention_4}
\end{equation}

Матрица $e$ служит для соответствия входа и выхода модели, оценивая, насколько хоошо соотносятся $i$-ый символ входной и $j$-ый символ выходной последовательностей.~\cite{attention}

Следующим прорывом в области NLP стало появление архитектуры трансформеров (рис. \ref{fig:transformer}) и переход к pre-trained моделям. На данный момент все state of the art модели в NLP так или иначе используют трансформеры. Их появление позволило создавать модели, состоящие из миллиардов параметров, обученные на сотнях GPU с использованием многих терабайтов данных. К наиболее популярным предобученным трансформерам относятся BERT, ALBERT, ROBERTA, ELMO, ERNIE, XLNET, GTP-2, T5 и другие.

\begin{figure}[ht]
	\centering
	\includegraphics{transformer}  
	\caption{ Структура блока трансформера}
	\label{fig:transformer}
\end{figure}

\subsection{N-грамные модели}
\label{sub:domain:n_gram}

Как уже было показано ранее, мы можем оценивать вероятности конкретных последовательностей по формуле (\ref{eq:conditional_proba}). Основная задача заключается в получении вероятностей $P(y_i|y_{<i})$. Наиболее очевидный способ --- узнать, какая часть последовательностей $y_1, y_2,\dots, y_{t-1}$ предшествует $y_t$ (\ref{eq:ngram}).
\begin{equation}
	P(y_t|y_1,y_2,\dots,y_{t-1}) = \frac{N(y_1,y_2,\dots,y_t)}{N(y_1,y_2,\dots,y_{t-1})},
	\label{eq:ngram}
\end{equation} где $N(y_1,\dots,y_k)$ -- количество раз, которое последовательность $y_1,\dots,y_k$ встречается в тексте. У данного способа довольно много недостатков: пересчитывать для последовательностей длиннее нескольких символов дорого, и значительная часть более длинных последовательностей в тексте не встречается, что даст нам нулевую вероятность. Чтобы этого избежать, сделаем предположение о Том, что текущее состояние зависит от контекста фиксированной длины. Формально это выражается в виде (1.7).
\begin{equation}
	P(y_t|y_1,y_2,\dots,y_{t-1}) = P(y_t|y_{t-n+1}, y_{t-n+2},\dots,y_{t-1})
\end{equation}
Для конкретных значений получим
\begin{itemize}
	\item[при] $n = 3$ (trigram): $P(y_t|y_1,y_2,\dots,y_{t-1}) = P(y_t|, y_{t-2},y_{t-1})$
	\item[при] $n = 2$ (bigram)): $P(y_t|y_1,y_2,\dots,y_{t-1}) = P(y_t|, y_{t-1})$
	\item[при] $n = 1$ (unigram): $P(y_t|y_1,y_2,\dots,y_{t-1}) = P(y_t)$
\end{itemize}

Далее можно столкнуться с проблемой, что либо числитель, либо знаменатель в формуле вероятности будет равен нулю. Для борьбы с нулем в знаменателе можно использовать:

\begin{itemize}
	\item Backoff, при котором мы просто уменьшаем контекст. То есть если $P(y_t|y_{t-n+1},\dots,y_{t-1}) = 0$, можно попробовать посчитать $P(y_t|y_{t-n+2},\dots,y_{t-1})$
	\item Linear interpolation. Это более качественный подход, в котором вероятность формируется сразу из нескольких n-gram. Например, для уни-, би- и триграм получим формулу $P(y_t|y_{t-2}, y_{t-1}) = \lambda_2P(y_t|y_{t-2}, y_{t-1}) + \lambda_1P(y_t|y_{t-1}) + \lambda_0P(y_t)$, где $\lambda_0 + \lambda_1 + \lambda_2 = 1$. Коэффициенты для взвешенной суммы можно подбирать кросс-валидацией на тренировочных данных.
\end{itemize}

Для борьбы с нулем в числителе применяется сглаживание. 

\subsubsection{Laplace smoothing}. Самый простой способ выполнить сглаживание --- добавить единицу ко всем счетчикам n-грамм, прежде чем
мы нормализуем их в вероятности. Этот алгоритм называется
Laplace smoothing. Laplace smoothing работает недостаточно хорошо, чтобы его можно было использовать в современных n-граммных моделях, но вводит многие концепции, которые успешно применяются в других статистических моделях, дает неплохой бейзлайн для дальнейшего языкового моделирования, а также используется на практике для других задач, таких как классификация текстовых документов.

Начнем с применения сглаживания Лапласа к вероятностям униграмм.
Напомним, что несглаженная оценка максимального правдоподобия вероятности униграммы является частным случаем для формулы (1.6) и имеет вид (\ref{eq:unigram_proba}):

\begin{equation}
	\label{eq:unigram_proba}
	P(w_i) = \frac{c_i}{N},
\end{equation}
\begin{explanation}
	где $w_i$ -- оцениваемое слово, $c_i$ -- сколько раз $w_i$ встречалось в тексте, \\$N$ -- общее число токенов.
\end{explanation}

Laplace Smoothing просто добавляет единицу к каждому счетчику (отсюда и его альтернативное название — сглаживание с добавлением единицы). Поскольку в словаре есть V слов, и каждое из них было увеличено, нам также необходимо скорректировать знаменатель, чтобы учесть дополнительное V (\ref{eq:laplace_unigram}).

\begin{equation}
	\label{eq:laplace_unigram}
	P(w_i) = \frac{c_i}{N},
\end{equation}

Вместо того, чтобы менять и числитель, и знаменатель, удобно
описать, как алгоритм сглаживания влияет на числитель, определив скорректированное значение счетчика $c_i$ по формуле \ref{eq:laplace_numerator}:

\begin{equation}
	\label{eq:laplace_numerator}
	c_i^* = (c_i + 1) \cdot \frac{N}{N + V}
\end{equation}

Сглаживание можно также рассматривать как это дисконтирование (понижение) некоторых ненулевых значений счетчиков для того, чтобы перераспределить вероятностную массу и получить ненулевые значения для слов, которые наша модель не знает. ~\cite{n_grams}

\subsubsection{Add-k}. Одной из альтернатив сглаживанию с добавлением единицы является перемещение немного меньшего количества вероятностной массы. Вместо того, чтобы добавлять 1 к каждому значению счетчика, мы добавляем некоторую величину $k$, которая обычно значительно меньше 1. Поэтому данный алгоритм называется Add-k smoothing (\ref{eq:add_k}).

\begin{equation}
	\label{eq:add_k}
	P^*_{Add-k}(w_n|w_{n-1}) = \frac{N(w_{n-1}w_n) + k}{N(w_{n-1}) + kV}
\end{equation}

Для использования Add-k сглаживания необходимо подбирать оптимальное значение k на тренировочных данных, например, при помощи кросс-валидации. несмотря на то, что add-k полезен для некоторых задач (включая классификацию текста), для языкового моделирования он все еще плох из-за больших дисперсий счетчиков и зачастую неуместного дисконтирования.

\subsubsection{Backoff и интерполяция}. Описанное выше дисконтирование позволяет решить проблему проблему n-грамм с нулевой вероятностью. Однако в таком случае мы используем не всю имеющуюся информацию о тренировочных данных, что значительно влияет на качество. Если мы пытаемся посчитать $P(w_n|w_{n-2}w_{n-1})$, но в тексте такая триграмма никогда не встречалась, то мы вместо нее можем использовать вероятность биниграммы $P(w_n|w_{n-1})$. Аналогично если нет данных о биграмме $P(w_n|w_{n-1})$, вместо нее применяем вероятность $P(w_n)$.

Другими словами, полезно использовать меньше контекста для генерализации последовательностей, которые наша модель не выучила. Есть два способа применять данный подход:

\begin{itemize}
	\item Backoff: используется триграмма, если она хоть раз встречалась в тренировочных данных, иначе используем биграмму и т. д. Мы итеративно снижаем контекст, если наша модель не имеет информации о данном контексте.
	\item Интерполяция: вместо одной n-граммы используется линейная комбинация нескольких n-грамм, с подобранными на тренировочных данных весами (\ref{eq:interpolation})
	\begin{equation}
		\hat{P}(w_n|w_{n-2}w_{n-1}) = \lambda_1 P(w_n) + \lambda_2 P(w_n|w_{n-1}) + \lambda_3 P(w_n|w_{n-2}w_{n-1})
		\label{eq:interpolation},
	\end{equation}
	\begin{explanation}
		где $\lambda_i$ -- вес i-ой n-граммы, $\sum_{i}{\lambda_i} = 1$
	\end{explanation}
\end{itemize}

В более сложной версии линейной интерполяции каждый вес $\lambda$ вычисляется в зависимости от контекста. Тогда если для какой-то биграммы у нас будут очень точно оценена ее вероятность, то мы полагаем, что вероятность триграммы будет более достоверная, и мы придаем ей больший вес при интерполяции (\ref{eq:improved_interpolation}).

\begin{equation}
	\begin{split}
	\hat{P}(w_n|w_{n-2}w_{n-1}) = \lambda_1(w_{n-2:n-1}) P(w_n) + \lambda_2(w_{n-2:n-1}) P(w_n|w_{n-1}) + \\ \lambda_3(w_{n-2:n-1}) P(w_n|w_{n-2}w_{n-1})
	\end{split}
	\label{eq:improved_interpolation}
\end{equation}

При таком подходе значения $\lambda$ вычисляются на отложенной выборке. Для этого довольно часто применяется EM алгоритм.

\subsubsection{Katz backoff}. Чтобы backoff модель давала правильное распределение вероятностей, мы вынуждены дисконтировать n-граммы более высокого порядка, чтобы сохранить некоторую вероятностную массу для n-грамм более низкого порядка. Как и в случае сглаживания с добавлением единицы, если n-граммы более высокого порядка не дисконтированные, и мы просто использовали недисконтированную вероятность MLE, то как только мы
заменили n-грамму с нулевой вероятностью n-граммой более низкого порядка, мы бы добавить массу вероятности, а общая вероятность для всех возможных строк будет больше 1. Поэтому, нам понадобится функция $\alpha$, чтобы распределить эту вероятностную массу на меньший порядок n-грамм. ~\cite{n_grams}

Один из видов такого перераспределения вероятностей называется Katz backoff. В данном алгоритме мы полагаемся на сглаженную вероятность P, если мы видели эту n-грамму раньше (т.е. если
у нас есть ненулевые значения). В противном случае мы рекурсивно уменьшаем контекст и смотрим на вероятность (N-1)-граммы. Значения при таком рекурсивеом спуске вычисляются по формуле (\ref{eq:maxim_katz}):

\begin{equation}
	P_{BO}(w_n|w_{n-N+1:n-1}) = 
	\begin{cases}
		P^*(w_n|w_{n-N+1:n-1}), & \mbox{if } C(w_{n-N+1:n}) > 0 \\ \alpha(w_{n-N+1:n-1})P_{BO}(w_n|w_{n-N+2:n-1}), & \mbox{otherwise.}
	\end{cases}
	\label{eq:maxim_katz}
\end{equation}

\subsubsection{Kneser-ney smoothing}. Одним из наиболее часто используемых и наиболее эффективных методов сглаживания n-грамм является алгоритм Kneser-Ney ~\cite{kneser_ney}. Kneser-Ney берет свое начало в методе, называемом абсолютным дисконтированием.

Интуиция, стоящая в основе абсолютного дисконтирования, заключается в том, что, поскольку у нас уже есть хорошие оценки для часто встрачающихся биграмм, небольшая скидка
$d$ на них особо не повлияет. При этом большим изменениям подвергнутся редкие биграммы, вероятностям которых мы и так не можем полностью доверять, и на практике, их снижение положительно сказывается на качестве модели. Уравнение абсолютной интерполяции выглядит следующим образом (\ref{eq:absolute_discounting}):

\begin{equation}
	P_{AD}(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) - d}{\sum_v{C(w_{i-1}, v)}} + \lambda(w_{i-1})P(w_i)
	\label{eq:absolute_discounting}
\end{equation}

Дисконтирование Кнезера-Нея ~\cite{kneser_ney} дополняет абсолютное дисконтирование более сложным способом обработки распределения униграмм низшего порядка. Вместо $P(w)$, что отвечает на вопрос «Насколько вероятно $w$?», мы хотели бы создать модель униграммы, которую назовем $P_{continuation}$, которая отвечает на вопрос «Насколько вероятно, что w появится в качестве продолжения?».  Интуиция Кнезера-Нея лежит в основе нашей оценки $P_{continuation}$ от количества различных контекстов, в которых появлялось слово w, то есть от количества биграмм, которые он завершает. Каждая биграмма была каким-то продолжением в новом контексте. Мы предполагаем, что слова, которые появлялись в большем количестве контекстов в
прошлом, скорее всего, также появится в каком-то новом контексте. Количество раз слово w появляется как новое продолжение, может быть выражено в виде (\ref{eq:extension_count}):

\begin{equation}
	P_{continuation}(w) \propto |{v: C(vw) > 0}|
	\label{eq:extension_count}
\end{equation}

Чтобы получить из данной величины вероятность, нормализуем ее по общему количеству известных модели биграмм (\ref{eq:extension_proba}).

\begin{equation}
	P_{continuation} = \frac{|{v:C(vw) > 0}|}{|{(u', w'): C(u', w') > 0}|}
	\label{eq:extension_proba}
\end{equation}

Аналогичным образом оценивается количество слов, которые предшествуют $w$ по формуле (\ref{eq:predicate_proba_or_smth_idk}).

\begin{equation}
	P_{continuation(w)} = \frac{|{v:C(vw) > 0}|}{\sum_{w'}{|{v:C(vw') > 0}|}}
	\label{eq:predicate_proba_or_smth_idk}
\end{equation}

Итоговое уравнение для Interpolated Kneser-Ney smoothing для биграмм принимает вид (\ref{eq:kneser_ney}):

\begin{equation}
	P_{KN}(w_i|w_{i-1}) = \frac{max(C(w_{i-1}w_i) - d, 0)}{C(w_{i-1})} + \lambda(w_{i-1}) P_{continuation}(w_i),
	\label{eq:kneser_ney}
\end{equation}
\begin{explanation}
	где $\lambda$ -- нормализующая константа, рассчитываемая по формуле (\ref{eq:lambda_for_KN}):
\end{explanation}

\begin{equation}
	\lambda(w_{i-1}) = \frac{d}{\sum_v{C(w_{i-1}v)}}|{w:C(w_{i-1}w) > 0}|
	\label{eq:lambda_for_KN},
\end{equation}

Первый член \ref{eq:lambda_for_KN} $\frac{d}{\sum_v{C(w_{i-1}v)}}$ -- коэффициент еормализации дисконтирования, второй член $|{w:C(w_{i-1}w) > 0}|$ -- количество различных слов, которые могут следовать за $w_{i-1}$, или количество раз, которое мы применяем дисконтирование.

Обобщенная формула Kneser-Ney имеет вид (\ref{eq:i_actualy_once_coded_this_shit}):

\begin{equation}
	P_{KN}(w_i|w_{i-n+1:i-1}) = \frac{max(c_{KN}(w_{i-n+1:i})-d, 0)}{\sum_v{c_{KN}(w_{i-n+1:i-1}v)}} + \\ \lambda(w_{i-n+1:i-1})P_{KN}(w_i|w_{i-n+2:i-1})
	\label{eq:i_actualy_once_coded_this_shit}
\end{equation}
\begin{explanation}
	где значение $c_{KN}$ зависит от того, считаем ли старшую n-грамму при \\ интерполяции (\ref{eq:KN_counts}).
\end{explanation}
\begin{equation}
	c_{KN}(\cdot) = \begin{cases} 
		count(\cdot), & \mbox{for the highest order }\\ 
		continuationcount(\cdot) & \mbox{for lower orders}
	\end{cases},
	\label{eq:KN_counts}
\end{equation}
\begin{explanation}
	где $continuationcount$ описывает количество уникальных однословных \\ контекстов для $\cdot$
\end{explanation}

При завершении рекурсии, униграммы интерполируются равномерным распределением (\ref{eq:KN_termination}):

\begin{equation}
	P_{KN}(w) = \frac{max(c_{KN}(w) - d, 0)}{\sum_{w'}{c_{KN}(w')}} + \lambda(\epsilon)\frac{1}{V},
	\label{eq:KN_termination}
\end{equation}
\begin{explanation}
	где $\epsilon$ описывает пустую строку.
\end{explanation}

Если мы хотим учитывать незнакомые слова $<UNK>$, то оно включается как обычный элемент словаря с количеством 0, и его вероятность будет по формуле (\ref{eq:KN_termination}) равна $\frac{\lambda{\epsilon}}{V}$.

Наилучшая версия алгоритма Kneser-Ney называется modified Kneser-Ney smoothing и описана в статье ~\cite{modified-kneser-ney}. Вместо использования одного значения $d$, в ней предлагается использовать три различных коэффициента дисконтирования $d_1$, $d_2$ и $d_{3+}$, что позволило превзойти начальную версию алгоритма. ~\cite{n_grams}

\subsection{Нейросетевые модели}
\label{sub:domain:neural}



\subsubsection{Сверточные сети}

\subsubsection{Рекуррентные сети}

\subsubsection{Self-attention и трансформеры}

\subsection{Современные подходы к языковому моделированию}
\label{sub:domain:curr}

\subsection{Метрики для языковых моделей}


